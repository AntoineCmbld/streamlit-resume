# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RMTpGmUxchQFwjlV-4Nl8bd8DRDlNtbB
"""

import streamlit as st

"""# Projet analyse des bandes de fréquences utilisées en France

## Import de la base de données
"""

import pandas as pd
import folium
from folium.plugins import HeatMap, MarkerCluster
import numpy as np
import streamlit as st

if (False):
    # Charger les données depuis le CSV
    df = pd.read_csv('https://github.com/PRAJAM/Projet/raw/jules/Export_TER_juin2023_FIX_SafwanChendeb.csv', sep=';')

    df.head()

    df2 = df.dropna(subset=['ASS_LGBD_KHZ'])
    df2 = df2[df2["ASS_LGBD_KHZ"] > 1]

    #Supressions des lignes d'antennes créées puis supprimées
    enreg_to_supp = df2[df2['MVT_CODE'] == 'SUP']['N° ENREG']
    df2 = df2[~df2['N° ENREG'].isin(enreg_to_supp)]

    #Tri par date et garder uniquement les derniers enregistrements pour chaque antenne
    df2['Date CAF'] = pd.to_datetime(df['Date CAF'], format='%d/%m/%Y')
    df2 = df2.sort_values('Date CAF').drop_duplicates(subset='N° ENREG', keep='last')

    # Fonction vectorisée pour convertir les coordonnées DMS en degrés décimaux
    def dms_to_dd(degrees, minutes, seconds, orientation):
        dd = degrees + minutes / 60 + seconds / 3600
        dd[orientation.isin(['W', 'S'])] *= -1
        return dd

    # Appliquer la conversion sur les colonnes de latitude et de longitude
    df2['latitude'] = dms_to_dd(df2['PT_LAT_DEG'], df2['PT_LAT_MIN'], df2['PT_LAT_SEC'], df2['PT_LAT_ORIENT'])
    df2['longitude'] = dms_to_dd(df2['PT_LONG_DEG'], df2['PT_LONG_MIN'], df2['PT_LONG_SEC'], df2['PT_LONG_ORIENT'])

    #drop
    df2 = df2.drop(['PT_LAT_DEG', 'PT_LAT_MIN', 'PT_LAT_SEC', 'PT_LAT_ORIENT', 'PT_LONG_DEG', 'PT_LONG_MIN', 'PT_LONG_SEC', 'PT_LONG_ORIENT', 'BASE', 'MVT_CODE', 'N° ENREG', 'TER_ANT_ANG', 'TER_ANT_AZM_MAX'], axis=1)

    df2.head()

    #drop ass_frq > 100 000 000
    df2 = df2[df2['ASS_FRQ_KHZ'] < 50000000]

    #drop ass_lgbd > 200 000
    df2 = df2[df2['ASS_LGBD_KHZ'] < 200000]

    #encadrer la longitude et latitude : 51.691467, -6.106181 et 41.924975, 8.307881 (France métropolitaine)
    df2 = df2[(df2['latitude'] >= 41.924975) & (df2['latitude'] <= 51.691467) & (df2['longitude'] >= -6.106181) & (df2['longitude'] <= 8.307881)]

    #show percentage of occurence for each code caf
    df2['Code CAF'].value_counts(normalize=True)

    #pickle df2
    df2.to_pickle("datasets/trbf.pkl")

df2 = pd.read_pickle("datasets/trbf.pkl")
dtypes = {
    'Code CAF': 'str',
    'ASS_FRQ_KHZ': 'int64',
    'ASS_LGBD_KHZ': 'float64',
    'Date CAF': 'str',
    'latitude': 'float64',
    'longitude': 'float64',
}

"""Après importation et préparation des données, voici les colonnes que nous analyseront :"""

description = pd.DataFrame(list(dtypes.items()), columns=["Nom de la colonne", "Type de données"])
info = pd.DataFrame({"Description" : ["Ce code correspond au service en charge de l'antenne. Il est anonymisé.", "Fréquence de l'antenne", "Largeur de la bande de fréquence", 
                                         "Date du dernier changement appliqué à l'antenne", "", ""
                                         ]})
description = pd.concat([description, info], axis=1)

st.table(description)

"""Voici une représentation avec heatmap des zones couvertes par des antennes"""



if (False):
    # Échantillonnage des données pour réduire le nombre de points
    sample_fraction = 0.1  # Vous pouvez ajuster cette valeur
    df_sample = df2.sample(frac=sample_fraction, random_state=1)

    # Créer une carte Folium centrée sur la France avec des tuiles légères
    m = folium.Map(location=[46.603354, 1.888334], zoom_start=6, tiles='CartoDB positron')

    # Ajouter des clusters de points pour une meilleure visualisation
    marker_cluster = MarkerCluster().add_to(m)

    # Ajouter les points au cluster
    for idx, row in df_sample.iterrows():
        folium.Marker(location=[row['latitude'], row['longitude']]).add_to(marker_cluster)

    # Préparer les données pour le HeatMap avec intensité
    heat_data = [[row['latitude'], row['longitude'], 1] for index, row in df_sample.iterrows()]

    # Ajouter le HeatMap à la carte avec des paramètres ajustés pour une meilleure performance et visibilité
    HeatMap(heat_data, radius=15, blur=10, max_zoom=12, min_opacity=0.4).add_to(m)

    # Sauvegarder la carte dans un fichier HTML
    m.save('antennas_heatmap_small.html')

#charger la carte html
html_file = open('antennas_heatmap_small.html', 'r')

# Afficher la carte dans le notebook
st.markdown(html_file.read(), unsafe_allow_html=True)

"""On remarque une forte concentration d'antennes dans les métropoles, ce qui peut s'expliquer par la densité de population et services, donc une forte demande.

Machine learning
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import MiniBatchKMeans
# Sélection des colonnes numériques et non numériques
numerical_cols = ['ASS_FRQ_KHZ', 'ASS_LGBD_KHZ',
                  'latitude', 'longitude']
categorical_cols = ['Code CAF']  # Exemple de colonnes catégorielles à encoder


# Encodage des colonnes catégorielles
label_encoder = LabelEncoder()

df_encoded = df2.copy()

for col in categorical_cols:
    df_encoded[col] = label_encoder.fit_transform(df_encoded[col])

# Normalisation des données
scaler = StandardScaler()

df_standardized = df_encoded.copy()

df_standardized[numerical_cols] = scaler.fit_transform(df_standardized[numerical_cols])
df_standardized[categorical_cols] = scaler.fit_transform(df_standardized[categorical_cols])
df_standardized['Date CAF'] = scaler.fit_transform(df_standardized[['Date CAF']])
df_standardized['Code CAF'] /= 3
df_standardized['Date CAF'] /= 2
df_standardized['Date CAF'] +=1.5

#plot each column
df_standardized.boxplot(figsize=(12, 8))

#matrice de correlation
import matplotlib.pyplot as plt
import seaborn as sns

correlation_matrix = df_standardized.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)

#import
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import MiniBatchKMeans

wcss = []
for i in range(1, 11):
    kmeans = MiniBatchKMeans(n_clusters=i, random_state=42)
    kmeans.fit(df_standardized)
    wcss.append(kmeans.inertia_)

# Tracé du coude (Elbow Method)
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.xlabel('Nombre de Clusters')
plt.ylabel('WCSS (Inertia)')
plt.title('Méthode du Coude pour K-Means')
plt.show()

df_standardized.head()
from sklearn.cluster import KMeans

#kmeans
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(df_standardized)

#boxplot each attribute with clusters
df_standardized['cluster'] = kmeans.labels_
df_standardized.boxplot(by='cluster', figsize=(12, 8))

#scatter df plotly
import plotly.express as px
fig = px.scatter(df2, x='Date CAF', y='ASS_FRQ_KHZ', color="Code CAF")
st.pyplot(fig)


#scatter df plotly
import plotly.express as px
fig = px.scatter(df2, x='Date CAF', y='ASS_FRQ_KHZ', color=kmeans.labels_)
st.pyplot(fig)


"""On remarque un cluster bien défini poour les antennes créées avant 1990. Cela s'explique par leur date et leurs fréquences assez basses par rapport à celles utilisées aujourd'hui."""

# prompt: do knn classification for code caf

from sklearn.neighbors import KNeighborsClassifier

# Define the features and target variables
X = df_standardized[['ASS_FRQ_KHZ', 'ASS_LGBD_KHZ', 'longitude', 'latitude', 'Date CAF']]
y = df2['Code CAF']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the KNN classifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Evaluate the KNN classifier
from sklearn.metrics import accuracy_score

y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

#f1 score
from sklearn.metrics import f1_score

f1 = f1_score(y_test, y_pred, average='weighted')
print('F1 Score:', f1)

#y_pred against y_test heat
from sklearn.metrics import confusion_matrix

confusion = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:\n', confusion)

# prompt: fais un random forest pour predir la frequence

from sklearn.ensemble import RandomForestRegressor

# Define the features and target variables
X = df_standardized[['ASS_LGBD_KHZ', 'longitude', 'latitude', 'Date CAF','Code CAF']]
y = df2['ASS_FRQ_KHZ']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Random Forest regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Evaluate the Random Forest regressor
y_pred = rf.predict(X_test)

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
print('Mean Squared Error:', mse)
#R^2 score
from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)
print('R^2 Score:', r2)
# Feature importance
importances = rf.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

# Print feature ranking
print("Feature ranking:")

for f in range(X.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances
from matplotlib import pyplot as plt
plt.figure()
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
        color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()

#plot ypred against y test
import matplotlib.pyplot as plt
plt.scatter(y_test, y_pred)
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# Convertir la colonne 'Date CAF' en format datetime
df2['Date CAF'] = pd.to_datetime(df2['Date CAF'], format='%d/%m/%Y')

# Filtrer les colonnes pertinentes pour l'analyse
df_filtered = df2[['Date CAF', 'ASS_FRQ_KHZ', 'ASS_LGBD_KHZ']]

# Ajouter une colonne 'Year' pour l'analyse
df_filtered['Year'] = df_filtered['Date CAF'].dt.year

# Grouper par année et calculer la moyenne annuelle pour les fréquences et les largeurs de bande
annual_trends = df_filtered.groupby('Year').mean().reset_index()

# Tracer les tendances temporelles
plt.figure(figsize=(14, 6))

# Tendance des fréquences assignées
plt.subplot(2, 1, 1)
plt.plot(annual_trends['Year'], annual_trends['ASS_FRQ_KHZ'], marker='o')
plt.title('Tendance des Fréquences Assignées par Année')
plt.xlabel('Année')
plt.ylabel('Fréquence Assignée (kHz)')

# Tendance des largeurs de bande
plt.subplot(2, 1, 2)
plt.plot(annual_trends['Year'], annual_trends['ASS_LGBD_KHZ'], marker='o', color='orange')
plt.title('Tendance des Largeurs de Bande par Année')
plt.xlabel('Année')
plt.ylabel('Largeur de Bande (kHz)')

plt.tight_layout()
plt.show()

# Préparer les séries temporelles
frequency_series = annual_trends.set_index('Year')['ASS_FRQ_KHZ']
bandwidth_series = annual_trends.set_index('Year')['ASS_LGBD_KHZ']

# Ajuster le modèle ARIMA pour les fréquences assignées
model_frq = ARIMA(frequency_series, order=(5, 1, 0))
model_frq_fit = model_frq.fit()

# Ajuster le modèle ARIMA pour les largeurs de bande
model_band = ARIMA(bandwidth_series, order=(5, 1, 0))
model_band_fit = model_band.fit()

# Faire des prédictions pour les 5 prochaines années
forecast_frq = model_frq_fit.forecast(steps=5)
forecast_band = model_band_fit.forecast(steps=5)

# Préparer les résultats pour l'affichage
forecast_years = list(range(annual_trends['Year'].max() + 1, annual_trends['Year'].max() + 6))
forecast_df = pd.DataFrame({
    'Year': forecast_years,
    'Forecast_Frq': forecast_frq,
    'Forecast_Band': forecast_band
})

# Afficher les résultats
forecast_df

"""Les antennes ont tendance à diffuser des ondes à fréquences de plus en plus élevéees, permettant un flux de données plus élevé."""